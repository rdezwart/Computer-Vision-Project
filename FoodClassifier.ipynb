{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPPUeWd8dEytpqnHyKilGia",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rdezwart/Computer-Vision-Project/blob/dev/FoodClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Food Classification Using YOLO"
      ],
      "metadata": {
        "id": "WVuWTDICLjsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Required Libraries\n"
      ],
      "metadata": {
        "id": "dQw5fOoHLtrE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GRXi1_FKLeaS"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsqJ2TLkL3sh",
        "outputId": "2134ebf6-48eb-4b93-c5b4-74b76128057c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nA16DVBQL8CA",
        "outputId": "4841f8f2-4d32-4531-c7af-75de90a75563"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 28.9/78.2 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "id": "MWqQ07ayMBYL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training YOLO"
      ],
      "metadata": {
        "id": "gklLBB1PMGgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('yolov8s-cls.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnOLA5O1MI_p",
        "outputId": "82b2ed9c-06b4-4807-9bfe-d6eacd76382b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8s-cls.pt to 'yolov8s-cls.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12.2M/12.2M [00:00<00:00, 344MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ignore this config file, turns out it's not needed for classification."
      ],
      "metadata": {
        "id": "BAzIGI9NaiP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.yaml\n",
        "path: /content/drive/MyDrive/2024/IAT_481/Datasets/Food_Images/food_db\n",
        "train: /content/drive/MyDrive/2024/IAT_481/Datasets/Food_Images/food_db/train\n",
        "test: /content/drive/MyDrive/2024/IAT_481/Datasets/Food_Images/food_db/test\n",
        "val: /content/drive/MyDrive/2024/IAT_481/Datasets/Food_Images/food_db/eval\n",
        "\n",
        "# Classes\n",
        "nc: 11 # replace based on your dataset's number of classes\n",
        "\n",
        "# Class names\n",
        "# replace all class names with your own classes' names\n",
        "names:\n",
        "  0: bread\n",
        "  1: dairy\n",
        "  2: dessert\n",
        "  3: egg\n",
        "  4: fried\n",
        "  5: meat\n",
        "  6: noodles\n",
        "  7: produce\n",
        "  8: rice\n",
        "  9: seafood\n",
        "  10: soup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSrboV-KM6wF",
        "outputId": "493d2e12-6a96-4245-ac63-df008c9418e9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(data=\"/content/drive/MyDrive/2024/IAT_481/Datasets/Food_Images/food_db\",epochs=1,batch=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kz8IIKsVN-en",
        "outputId": "02c49314-0a3c-4a79-c1ba-fb83d008a182"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8s-cls.pt, data=/content/drive/MyDrive/2024/IAT_481/Datasets/Food_Images/food_db, epochs=1, time=None, patience=100, batch=16, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train2\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/2024/IAT_481/Datasets/Food_Images/food_db/train... found 9866 images in 11 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/drive/MyDrive/2024/IAT_481/Datasets/Food_Images/food_db/val... found 3347 images in 11 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/drive/MyDrive/2024/IAT_481/Datasets/Food_Images/food_db/test... found 3430 images in 11 classes âœ… \n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    672011  ultralytics.nn.modules.head.Classify         [512, 11]                     \n",
            "YOLOv8s-cls summary: 99 layers, 5094827 parameters, 5094827 gradients, 12.6 GFLOPs\n",
            "Transferred 158/158 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train2', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/2024/IAT_481/Datasets/Food_Images/food_db/train... 9866 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9866/9866 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/2024/IAT_481/Datasets/Food_Images/food_db/val... 3347 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3347/3347 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 224 train, 224 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/classify/train2\u001b[0m\n",
            "Starting training for 1 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/1     0.512G      2.464         16        224:   1%|          | 7/617 [00:02<02:53,  3.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/1     0.512G      2.442         16        224:   2%|â–         | 11/617 [00:04<04:16,  2.37it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 95.3MB/s]\n",
            "        1/1     0.528G      1.422         10        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 617/617 [1:02:11<00:00,  6.05s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [16:10<00:00,  9.25s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all       0.84      0.992\n",
            "\n",
            "1 epochs completed in 1.308 hours.\n",
            "Optimizer stripped from runs/classify/train2/weights/last.pt, 10.3MB\n",
            "Optimizer stripped from runs/classify/train2/weights/best.pt, 10.3MB\n",
            "\n",
            "Validating runs/classify/train2/weights/best.pt...\n",
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8s-cls summary (fused): 73 layers, 5089291 parameters, 0 gradients, 12.5 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/2024/IAT_481/Datasets/Food_Images/food_db/train... found 9866 images in 11 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/drive/MyDrive/2024/IAT_481/Datasets/Food_Images/food_db/val... found 3347 images in 11 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/drive/MyDrive/2024/IAT_481/Datasets/Food_Images/food_db/test... found 3430 images in 11 classes âœ… \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:42<00:00,  2.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all       0.84      0.992\n",
            "Speed: 0.1ms preprocess, 0.5ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/classify/train2\u001b[0m\n",
            "Results saved to \u001b[1mruns/classify/train2\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
              "\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7fbf48421b10>\n",
              "curves: []\n",
              "curves_results: []\n",
              "fitness: 0.9160442650318146\n",
              "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
              "results_dict: {'metrics/accuracy_top1': 0.8398566246032715, 'metrics/accuracy_top5': 0.9922319054603577, 'fitness': 0.9160442650318146}\n",
              "save_dir: PosixPath('runs/classify/train2')\n",
              "speed: {'preprocess': 0.0909292274067072, 'inference': 0.49147023205832646, 'loss': 0.0007161817730167641, 'postprocess': 0.0005635995810730691}\n",
              "task: 'classify'\n",
              "top1: 0.8398566246032715\n",
              "top5: 0.9922319054603577"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Metrics"
      ],
      "metadata": {
        "id": "aNnFtI9Ec00r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = model.val()  # no arguments needed, dataset and settings remembered"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucMnF_5BdA6j",
        "outputId": "7fb73b76-8806-42e9-aa1f-f4a9de961a7c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/2024/IAT_481/Datasets/Food_Images/food_db/train... found 9866 images in 11 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/drive/MyDrive/2024/IAT_481/Datasets/Food_Images/food_db/val... found 3347 images in 11 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/drive/MyDrive/2024/IAT_481/Datasets/Food_Images/food_db/test... found 3430 images in 11 classes âœ… \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/2024/IAT_481/Datasets/Food_Images/food_db/val... 3347 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3347/3347 [00:00<?, ?it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 210/210 [00:43<00:00,  4.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all       0.84      0.992\n",
            "Speed: 0.1ms preprocess, 1.0ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/classify/train25\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "c2b30qM1FFtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = model()"
      ],
      "metadata": {
        "id": "DAts3TqwFHTu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}